{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.dataframe import from_pandas\n",
    "from dask.dataframe.utils import make_meta\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import ClientError\n",
    "from dask.distributed import Client, LocalCluster, get_worker\n",
    "import dask\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Miners\n",
    "from pm4py import serialize, deserialize\n",
    "from pm4py import discover_dfg as dfg_discovery\n",
    "from pm4py.discovery import DFG\n",
    "\n",
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py import discover_petri_net_inductive as inductive_miner\n",
    "\n",
    "\n",
    "# Evaluators\n",
    "\n",
    "from pm4py import fitness_token_based_replay as fitness_token_based_replay #fitness\n",
    "from pm4py import precision_alignments as precision_token_based_replay #precision\n",
    "# from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator #simplicity\n",
    "# from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator #generalization\n",
    "# from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "# from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
    "# from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "# from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf08aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({'distributed.scheduler.active-memory-manager.start': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15189d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc779c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_driver():\n",
    "    def __init__(self, uri_scheme='bolt', host='localhost', port='7687', username='neo4j', password='123456'):\n",
    "        self.uri_scheme = uri_scheme\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        \n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        \n",
    "        self.connection_uri = \"{uri_scheme}://{host}:{port}\".format(uri_scheme=self.uri_scheme, host=self.host, port=self.port)\n",
    "        self.auth = (self.username, self.password)\n",
    "        self.driver = GraphDatabase.driver(self.connection_uri, auth=self.auth)\n",
    "        \n",
    "    def __del__(self):\n",
    "        self._close_driver()\n",
    "    \n",
    "    def _close_driver(self):\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "    \n",
    "    def run_single_query(self, query):\n",
    "        res = None\n",
    "        with self.driver.session() as session:\n",
    "            raw_res = session.run(query)\n",
    "            res = self.format_raw_res(raw_res)\n",
    "        return res\n",
    "    \n",
    "    def run_bulk_query(self, query_list):\n",
    "        results = []\n",
    "        with self.driver.session() as session:\n",
    "            for query in tqdm(query_list):\n",
    "                raw_res = session.run(query)\n",
    "                res = self.format_raw_res(raw_res)\n",
    "                results.append({'query':query, 'result':res})\n",
    "        return results\n",
    "    \n",
    "    def reset_graph(self, db=None):\n",
    "        return self.run_single_query(\"MATCH (n) DETACH DELETE n\")\n",
    "    \n",
    "    def test_connection(self):\n",
    "        return self.run_single_query(\"MATCH (n) RETURN COUNT(n) as nodes\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_raw_res(raw_res):\n",
    "        res = []\n",
    "        for r in raw_res:\n",
    "            res.append(r)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ede6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def useExecutionTime(func):\n",
    "    \n",
    "    def compute(*args, **kwargs):\n",
    "        begin = time.time()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        return {\"result\": result, \"execution_time\": end - begin}\n",
    " \n",
    "    return compute\n",
    "\n",
    "@useExecutionTime\n",
    "def getComputeTime(*args, **kwargs):\n",
    "    return dask.compute(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=1, threads_per_worker=1, memory_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gc(dask_worker,**kwargs):\n",
    "    gc.collect()\n",
    "    return True\n",
    "\n",
    "# Register the GC function as a plugin\n",
    "client.register_worker_plugin(run_gc, \"my_gc_plugin\")\n",
    "client.register_worker_plugin(trim_memory, \"my_trim_plugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=1, maximum=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnTypes = {\n",
    "    'case:IDofConceptCase': 'string',\n",
    "    'case:Includes_subCases': 'string',\n",
    "    'case:Responsible_actor': 'string',\n",
    "    'case:caseProcedure': 'string',\n",
    "    'case:concept:name': 'int64',\n",
    "    'dueDate': 'string',\n",
    "    'case:termName': 'string',\n",
    "    'dateStop': 'string',\n",
    "    'case:endDate': 'object',\n",
    "    'case:endDatePlanned': 'object',\n",
    "    'case:parts': 'object'\n",
    "}\n",
    "\n",
    "# list of file paths to be loaded\n",
    "file_paths = ['BPIC15_1.csv']\n",
    "\n",
    "# load the first file as a Dask dataframe\n",
    "df = dd.read_csv(file_paths[0], dtype=columnTypes)\n",
    "\n",
    "# iterate over the remaining files\n",
    "for file_path in file_paths[1:]:\n",
    "    # usecols parameter to load only the columns that are present in both dataframes\n",
    "    df_temp = dd.read_csv(file_path)\n",
    "    # concatenate the dataframes along the rows\n",
    "    df = dd.concat([df, dd.read_csv(file_path, dtype=columnTypes)], interleave_partitions=True)\n",
    "\n",
    "# columnTypes = {\n",
    "#     'OfferID': 'string'\n",
    "# }\n",
    "\n",
    "# fileName = 'BPI Challenge 2017'\n",
    "# df = dd.read_csv('{fileName}.csv'.format(fileName=fileName), dtype=columnTypes)\n",
    "for column in df.columns:\n",
    "    if re.search(\"[Dd]ate.*|time.*\", column):\n",
    "        df[column] = dask.dataframe.to_datetime(df[column], utc=True)\n",
    "        \n",
    "# df['case:concept:name'] = df['case:concept:name'].replace(to_replace=\"Application_\", value='', regex=True)\n",
    "df['case:concept:name'] = df['case:concept:name'].astype({'case:concept:name': 'int64'})\n",
    "        \n",
    "df = df.repartition(npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformToDFG(dfgResult):\n",
    "    result = {}\n",
    "    for record in dfgResult:\n",
    "        result[(record[\"parent\"], record[\"child\"])] = record[\"frequency\"]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def transformToStartEndActivity(activities):\n",
    "    result = {}\n",
    "    for record in activities:\n",
    "        result[record['name']] = record[\"frequency\"]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08471450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDFG():\n",
    "    queries = {\n",
    "        \"dfgQuery\": \"\"\"MATCH result=(p:Activity)-[r:PRODUCES]->(c:Activity) RETURN p.name as parent, c.name as child, r.frequency as frequency\"\"\",\n",
    "        \"startEndActivitiesQuery\": [\"MATCH (a:StartActivity) RETURN a.name as name , a.frequency as frequency\", \"MATCH (a:EndActivity) RETURN a.name as name , a.frequency as frequency\"],\n",
    "    }\n",
    "    \n",
    "    neo4jConnection = graph_driver(uri_scheme=\"neo4j\",host=\"neo4j\", password=\"123456\")\n",
    "    \n",
    "    dfgResult = neo4jConnection.run_single_query(queries['dfgQuery'])\n",
    "    startEndActivitiesResult = neo4jConnection.run_bulk_query(queries['startEndActivitiesQuery'])\n",
    "    return [transformToDFG(dfgResult), transformToStartEndActivity(startEndActivitiesResult[0][\"result\"]), transformToStartEndActivity(startEndActivitiesResult[1][\"result\"])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_df = df.set_index('case:concept:name', drop=False, sorted=True)\n",
    "indexed_df['case:concept:name'] = indexed_df['case:concept:name'].astype({'case:concept:name': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc70730",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_df.index = indexed_df.index.rename('caseId')\n",
    "indexed_df = indexed_df.repartition(npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg, start, end = getDFG()\n",
    "dfgObj = DFG(dfg, start_activities=start, end_activities=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@useExecutionTime\n",
    "def getMinerResult(dfg, miner, threshold = 0.5):\n",
    "    result = {}\n",
    "    if miner == 'heuristic_miner':\n",
    "        net, im, fm = heuristics_miner.apply_dfg(dfg['dfg'], parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: threshold})\n",
    "    elif miner == 'inductive_miner':\n",
    "        net, im, fm = inductive_miner(dfg['dfgObj'])\n",
    "    elif miner == 'alpha_miner':\n",
    "        net, im, fm = alpha_miner.apply_dfg(dfg['dfg'])\n",
    "    \n",
    "    result[miner] = serialize(net, im, fm)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def setLazyMiners(dfg):\n",
    "    lazyList = []\n",
    "    miners = [\n",
    "        'heuristic_miner',\n",
    "        'inductive_miner',\n",
    "#         'alpha_miner'\n",
    "    ]\n",
    "    for miner in miners:\n",
    "        task = dask.delayed(getMinerResult)(dfg, miner)\n",
    "        lazyList.append(task)\n",
    "    \n",
    "    return lazyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ef81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@useExecutionTime\n",
    "def getMetrics(log, miner, metric, net, im, fm):\n",
    "    try:\n",
    "        result = {\n",
    "            miner: {\n",
    "                metric: 0\n",
    "            }\n",
    "        }\n",
    "        if metric == 'fitness':\n",
    "            result[miner][metric] = fitness_token_based_replay(log, net, im, fm)\n",
    "        elif metric == 'simplicity':\n",
    "            result[miner][metric] = simplicity_evaluator.apply(net)\n",
    "        elif metric == 'precision':\n",
    "            result[miner][metric] = precision_token_based_replay(log, net, im, fm,  activity_key='concept:name', case_id_key='case:concept:name', timestamp_key='time:timestamp')\n",
    "        elif metric == 'generalization':\n",
    "            result[miner][metric] = generalization_evaluator.apply(log, net, im, fm)\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {miner: {metric: {\"error\": e}}}\n",
    "\n",
    "def setLazyMetrics(log, miners):\n",
    "    lazyList = []\n",
    "    metrics = [\n",
    "#         'fitness',\n",
    "#         'simplicity',\n",
    "        'precision',\n",
    "#         'generalization'\n",
    "    ]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for miner in miners:\n",
    "            algorithm = list(miner['result'].keys())[0]\n",
    "            net, im, fm = deserialize(miner['result'][algorithm])\n",
    "            task = getMetrics(log, algorithm, metric, net, im, fm)\n",
    "            lazyList.append(task)\n",
    "    \n",
    "    return lazyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26251e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazyMiners = setLazyMiners({\"dfgObj\": dfgObj, \"dfg\": dfg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f366c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazyMinersResults = dask.compute(*lazyMiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazyMetrics = setLazyMetrics(indexed_df, lazyMinersResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649993ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = indexed_df.map_partitions(setLazyMetrics, lazyMinersResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823036b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indexed_df.get_partition(n=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a14774",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indexed_df.get_partition(n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbf745",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indexed_df.get_partition(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdPartition = indexed_df.get_partition(n=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdPartitionResult = setLazyMetrics(thirdPartition.compute(), lazyMinersResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdPartitionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07288608",
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdPartitionResult.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6683eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazyMetricsResults = client.compute(lazyMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('{fileName}_miners_results.dill'.format(fileName=' '.join(file_paths)), 'wb') as file:\n",
    "#     dill.dump(lazyMinersResults, file)\n",
    "\n",
    "# with open('{fileName}_evaluation_results.dill'.format(fileName=' '.join(file_paths)), 'wb') as file:\n",
    "#     dill.dump(lazyMetricsResults, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
