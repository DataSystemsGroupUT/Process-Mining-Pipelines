{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0df7238",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.dataframe import from_pandas\n",
    "from dask.dataframe.utils import make_meta\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import ClientError\n",
    "from dask.distributed import Client, LocalCluster, get_worker\n",
    "import dask\n",
    "\n",
    "import os\n",
    "import time\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import dill\n",
    "\n",
    "# Miners\n",
    "from pm4py import serialize, deserialize\n",
    "from pm4py import discover_dfg as dfg_discovery\n",
    "from pm4py.discovery import DFG\n",
    "\n",
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py import discover_petri_net_inductive as inductive_miner\n",
    "\n",
    "\n",
    "# Evaluators\n",
    "from contribution import fitness_alignment, precision_alignment, generalization\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator #simplicity\n",
    "from pm4py.objects.petri_net.utils.check_soundness import check_easy_soundness_net_in_fin_marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf08aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0xffffa476dd20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dask.config.set({'distributed.scheduler.active-memory-manager.start': True, 'distributed.comm.timeouts.tcp': '7200s'})\n",
    "dask.config.set({\"distributed.serializer\": \"dill\"})\n",
    "# dask.config.set(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15189d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158c668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc779c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_driver():\n",
    "    def __init__(self, uri_scheme='bolt', host='localhost', port='7687', username='neo4j', password='123456'):\n",
    "        self.uri_scheme = uri_scheme\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        \n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        \n",
    "        self.connection_uri = \"{uri_scheme}://{host}:{port}\".format(uri_scheme=self.uri_scheme, host=self.host, port=self.port)\n",
    "        self.auth = (self.username, self.password)\n",
    "        self.driver = GraphDatabase.driver(self.connection_uri, auth=self.auth)\n",
    "        \n",
    "    def __del__(self):\n",
    "        self._close_driver()\n",
    "    \n",
    "    def _close_driver(self):\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "    \n",
    "    def run_single_query(self, query):\n",
    "        res = None\n",
    "        with self.driver.session() as session:\n",
    "            raw_res = session.run(query)\n",
    "            res = self.format_raw_res(raw_res)\n",
    "        return res\n",
    "    \n",
    "    def run_bulk_query(self, query_list):\n",
    "        results = []\n",
    "        with self.driver.session() as session:\n",
    "            for query in tqdm(query_list):\n",
    "                raw_res = session.run(query)\n",
    "                res = self.format_raw_res(raw_res)\n",
    "                results.append({'query':query, 'result':res})\n",
    "        return results\n",
    "    \n",
    "    def reset_graph(self, db=None):\n",
    "        return self.run_single_query(\"MATCH (n) DETACH DELETE n\")\n",
    "    \n",
    "    def test_connection(self):\n",
    "        return self.run_single_query(\"MATCH (n) RETURN COUNT(n) as nodes\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_raw_res(raw_res):\n",
    "        res = []\n",
    "        for r in raw_res:\n",
    "            res.append(r)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ede6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def useExecutionTime(func):\n",
    "    \n",
    "    def compute(*args, **kwargs):\n",
    "        begin = time.time()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        return {\"result\": result, \"execution_time\": end - begin}\n",
    " \n",
    "    return compute\n",
    "\n",
    "@useExecutionTime\n",
    "def getComputeTime(*args, **kwargs):\n",
    "    return dask.compute(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd1a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCluster(n_workers=1, threads_per_worker=1, memory_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b9bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(cluster)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af7e6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gc(dask_worker,**kwargs):\n",
    "    gc.collect()\n",
    "    return True\n",
    "\n",
    "# Register the GC function as a plugin\n",
    "# client.register_worker_plugin(run_gc, \"my_gc_plugin\")\n",
    "# client.register_worker_plugin(trim_memory, \"my_trim_plugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ba2b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.adapt(minimum=1, maximum=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ae56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnTypes = {\n",
    "#     'case:IDofConceptCase': 'string',\n",
    "#     'case:Includes_subCases': 'string',\n",
    "#     'case:Responsible_actor': 'string',\n",
    "#     'case:caseProcedure': 'string',\n",
    "#     'case:concept:name': 'int64',\n",
    "#     'dueDate': 'object',\n",
    "#     'case:termName': 'string',\n",
    "#     'dateStop': 'object',\n",
    "#     'case:endDate': 'object',\n",
    "#     'case:endDatePlanned': 'object',\n",
    "#     'case:parts': 'object',\n",
    "#     'msgCode': 'string',\n",
    "#     'msgType': 'string',\n",
    "#     'case:landRegisterID': 'object'\n",
    "}\n",
    "\n",
    "# list of file paths to be loaded\n",
    "\n",
    "file_paths = ['BPI_2020_InternationalDeclarations']\n",
    "\n",
    "# load the first file as a Dask dataframe\n",
    "df = dd.read_csv('{}.csv'.format(file_paths[0]), dtype=columnTypes, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# iterate over the remaining files\n",
    "for file_path in file_paths[1:]:\n",
    "    # usecols parameter to load only the columns that are present in both dataframes\n",
    "    df_temp = dd.read_csv('{}.csv'.format(file_path), dtype=columnTypes)\n",
    "    # concatenate the dataframes along the rows\n",
    "    df = dd.concat([df, df_temp], interleave_partitions=True)\n",
    "\n",
    "# columnTypes = {\n",
    "#     'OfferID': 'string'\n",
    "# }\n",
    "\n",
    "\n",
    "# BPI 2014 sep=';'\n",
    "# df = df.rename(columns={\"Incident ID\": \"case:concept:name\", \"IncidentActivity_Type\": \"concept:name\", \"DateStamp\": \"time:timestamp\"})\n",
    "\n",
    "# df = df.rename(columns={\"case concept:name\": \"case:concept:name\", \"event concept:name\": \"concept:name\", \"event time:timestamp\": \"time:timestamp\"})\n",
    "for column in df.columns:\n",
    "    if re.search(\"[Dd]ate.*|time.*\", column):\n",
    "        df[column] = dask.dataframe.to_datetime(df[column], utc=True)\n",
    "\n",
    "df['case:concept:name'] = df['case:concept:name'].replace(to_replace=\"[a-zA-Z]\", value='', regex=True)\n",
    "df['case:concept:name'] = df['case:concept:name'].astype('int')\n",
    "        \n",
    "df = df.repartition(npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "470c35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformToDFG(dfgResult):\n",
    "    result = {}\n",
    "    for record in dfgResult:\n",
    "        result[(record[\"parent\"], record[\"child\"])] = record[\"frequency\"]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def transformToStartEndActivity(activities):\n",
    "    result = {}\n",
    "    for record in activities:\n",
    "        result[record['name']] = record[\"frequency\"]\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08471450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDFG():\n",
    "    queries = {\n",
    "        \"dfgQuery\": \"\"\"MATCH result=(p:Activity)-[r:PRODUCES]->(c:Activity) RETURN p.name as parent, c.name as child, r.frequency as frequency\"\"\",\n",
    "        \"startEndActivitiesQuery\": [\"MATCH (a:StartActivity) RETURN a.name as name , a.frequency as frequency\", \"MATCH (a:EndActivity) RETURN a.name as name , a.frequency as frequency\"],\n",
    "    }\n",
    "    \n",
    "    neo4jConnection = graph_driver(uri_scheme=\"neo4j\",host=\"neo4j\", password=\"123456\")\n",
    "    \n",
    "    dfgResult = neo4jConnection.run_single_query(queries['dfgQuery'])\n",
    "    startEndActivitiesResult = neo4jConnection.run_bulk_query(queries['startEndActivitiesQuery'])\n",
    "    return [transformToDFG(dfgResult), transformToStartEndActivity(startEndActivitiesResult[0][\"result\"]), transformToStartEndActivity(startEndActivitiesResult[1][\"result\"])]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa32b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_df = df.set_index('case:concept:name', drop=False, sorted=True)\n",
    "indexed_df['case:concept:name'] = indexed_df['case:concept:name'].astype({'case:concept:name': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "582f9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acc70730",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_df.index = indexed_df.index.rename('caseId')\n",
    "indexed_df = indexed_df.repartition(npartitions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c59ea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2/2 [00:00<00:00, 271.29it/s]\n"
     ]
    }
   ],
   "source": [
    "dfg, start, end = getDFG()\n",
    "dfgObj = DFG(dfg, start_activities=start, end_activities=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1d07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@useExecutionTime\n",
    "def getMinerResult(dfg, miner):\n",
    "    result = {}\n",
    "    if miner == 'heuristic_miner':\n",
    "        net, im, fm = heuristics_miner.apply_dfg(dfg['dfg'])\n",
    "    elif miner == 'inductive_miner':\n",
    "        net, im, fm = inductive_miner(dfg['dfgObj'])\n",
    "    elif miner == 'alpha_miner':\n",
    "        net, im, fm = alpha_miner.apply_dfg(dfg['dfg'])\n",
    "    \n",
    "    result[miner] = serialize(net, im, fm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return result\n",
    "    \n",
    "def setLazyMiners(dfg):\n",
    "    lazyList = []\n",
    "    miners = [\n",
    "        'heuristic_miner',\n",
    "        'inductive_miner',\n",
    "        'alpha_miner'\n",
    "    ]\n",
    "    for miner in miners:\n",
    "        task = dask.delayed(getMinerResult)(dfg, miner)\n",
    "        lazyList.append(task)\n",
    "    \n",
    "    return lazyList\n",
    "\n",
    "def reformatMinersResults(lazyMinersResults):\n",
    "    minersResults = {}\n",
    "    for result in lazyMinersResults:\n",
    "        miner = list(result['result'].keys())[0]\n",
    "        minersResults[miner] = {}\n",
    "        net, im, fm = deserialize(result['result'][miner])\n",
    "        minersResults[miner] = {\n",
    "            'net': net,\n",
    "            'im': im,\n",
    "            'fm': fm,\n",
    "            'execution_time': result['execution_time']\n",
    "        }\n",
    "    return minersResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "163ef81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@useExecutionTime\n",
    "def getMetrics(log, miner, metric, net, im, fm):\n",
    "    sys.setrecursionlimit(3000)\n",
    "    try:\n",
    "        result = {}\n",
    "        result.setdefault(miner, {})\n",
    "        result[miner].setdefault(metric, 0)\n",
    "        if metric == 'fitness':\n",
    "            result[miner][metric] = fitness_alignment.apply(log, net, im, fm)\n",
    "        elif metric == 'simplicity':\n",
    "            result[miner][metric] = simplicity_evaluator.apply(net)\n",
    "        elif metric == 'precision':\n",
    "            result[miner][metric] = precision_alignment.apply(log, net, im, fm)\n",
    "        elif metric == 'generalization':\n",
    "            result[miner][metric] = generalization.apply(log, net, im, fm)\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {miner: {metric: {\"error\": e}}}\n",
    "\n",
    "def setLazyMetrics(log, miners):\n",
    "    lazyList = []\n",
    "    metrics = [\n",
    "#         'fitness',\n",
    "#         'simplicity',\n",
    "        'precision',\n",
    "#         'generalization'\n",
    "    ]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for miner in miners.keys():\n",
    "            \n",
    "            net, im, fm = [miners[miner]['net'], miners[miner]['im'], miners[miner]['fm']]\n",
    "            task = getMetrics(log, miner, metric, net, im, fm)\n",
    "            lazyList.append(task)\n",
    "    \n",
    "    return lazyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26251e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazyMiners = setLazyMiners({\"dfgObj\": dfgObj, \"dfg\": dfg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f366c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazyMinersResults = reformatMinersResults(dask.compute(*lazyMiners, scheduler='processes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e332366",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def aggregate(partitions):\n",
    "    import sys\n",
    "    sys.setrecursionlimit(10**5)\n",
    "    result = {}\n",
    "    \n",
    "    for partition in partitions:\n",
    "        for output in partition:\n",
    "            miner = list(output['result'].keys())[0]\n",
    "            metric = list(output['result'][miner].keys())[0]\n",
    "            e_time = output['execution_time']\n",
    "            \n",
    "            result.setdefault(miner, {})\n",
    "            result[miner].setdefault(metric, {})\n",
    "            result[miner][metric].setdefault('result', None)\n",
    "            result[miner][metric].setdefault('execution_time', 0)\n",
    "            \n",
    "            result[miner][metric]['execution_time'] = e_time\n",
    "            \n",
    "            if result[miner][metric]['result'] == None:\n",
    "                result[miner][metric]['result'] = output['result'][miner][metric]\n",
    "                continue\n",
    "    \n",
    "            if metric and metric == 'fitness':\n",
    "                result[miner][metric]['result'] = fitness_alignment.aggregate(output['result'][miner][metric], result[miner][metric]['result'])\n",
    "            elif metric and metric == 'precision':\n",
    "                result[miner][metric]['result'] = precision_alignment.aggregate(output['result'][miner][metric], result[miner][metric]['result'])\n",
    "            if metric and metric == 'generalization':\n",
    "                result[miner][metric]['result'] = generalization.aggregate([output['result'][miner][metric], result[miner][metric]['result']])\n",
    "\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8f27745",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def compute_metrics(aggregatedMetrics, minersResults):\n",
    "    \n",
    "    import sys\n",
    "    sys.setrecursionlimit(10**5)\n",
    "    \n",
    "    results = {}    \n",
    "    for miner, metrics in aggregatedMetrics.items():\n",
    "        net, im, fm = [minersResults[miner]['net'], minersResults[miner]['im'], minersResults[miner]['fm']]\n",
    "        for metricKey, metricValue in metrics.items():\n",
    "            results.setdefault(miner, {})\n",
    "            results[miner].setdefault(metricKey, {})\n",
    "            results[miner][metricKey].setdefault('result', None)\n",
    "            results[miner][metricKey].setdefault('execution_time', 0)\n",
    "            \n",
    "            results[miner][metricKey]['execution_time'] = metricValue['execution_time']\n",
    "            start_time = timeit.default_timer()\n",
    "            if not check_easy_soundness_net_in_fin_marking(net, im, fm) and metricKey == 'precision':\n",
    "                results[miner][metricKey]['result'] = 'NA'\n",
    "                continue\n",
    "            if metricKey and metricKey == 'fitness':\n",
    "                results[miner][metricKey]['result'] = fitness_alignment.compute(metricValue['result'], net=net, im=im, fm=fm)\n",
    "            elif metricKey and metricKey == 'precision':\n",
    "                results[miner][metricKey]['result'] = precision_alignment.compute(**metricValue['result'], net=net, im=im, fm=fm)\n",
    "            elif metricKey and metricKey == 'generalization':\n",
    "                results[miner][metricKey]['result'] = generalization.compute(metricValue['result']['trans_occ_map'], minersResults[miner]['net'])\n",
    "            elif metricKey and metricKey == 'simplicity':\n",
    "                results[miner][metricKey]['result'] = simplicity_evaluator.apply(net)\n",
    "            end_time = timeit.default_timer()\n",
    "            results[miner][metricKey]['execution_time'] = sum([(end_time - start_time), metricValue['execution_time']])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "649993ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data = indexed_df.map_partitions(setLazyMetrics, lazyMinersResults, meta=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "331a663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = aggregate(mapped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b141af",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = compute_metrics(aggregated_results, lazyMinersResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83420c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pm4py/algo/evaluation/precision/utils.py:121: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  traces = list(log.groupby(case_id_key)[activity_key].apply(tuple))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 9.61 ss\n"
     ]
    }
   ],
   "source": [
    "results = dask.compute(r, scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e6b5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedResult = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c78dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b319445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heuristic_miner</th>\n",
       "      <th>inductive_miner</th>\n",
       "      <th>alpha_miner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'precision': {'result': 'NA', 'execution_time': 0.0012431144714355469}}</td>\n",
       "      <td>{'precision': {'result': 0.10602897118456533, 'execution_time': 9.917851378209889}}</td>\n",
       "      <td>{'precision': {'result': 'NA', 'execution_time': 0.000667572021484375}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            heuristic_miner  \\\n",
       "0  {'precision': {'result': 'NA', 'execution_time': 0.0012431144714355469}}   \n",
       "\n",
       "                                                                       inductive_miner  \\\n",
       "0  {'precision': {'result': 0.10602897118456533, 'execution_time': 9.917851378209889}}   \n",
       "\n",
       "                                                               alpha_miner  \n",
       "0  {'precision': {'result': 'NA', 'execution_time': 0.000667572021484375}}  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24eab6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStatisticalDataFrames(minersResults):\n",
    "\n",
    "    metricsExecutionTimePerMiner = {}\n",
    "            \n",
    "    for miner in minersResults.keys():\n",
    "        execution_time = minersResults[miner]['execution_time']\n",
    "        metricsExecutionTimePerMiner.setdefault(miner, execution_time)\n",
    "        metricsExecutionTimePerMiner[miner] = execution_time\n",
    "            \n",
    "    metricsExecutionTimePerMiner['data_set'] = '-'.join(file_paths)\n",
    "            \n",
    "    return pd.DataFrame(metricsExecutionTimePerMiner, index=['execution_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb95fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "miner_execution_time = getStatisticalDataFrames(lazyMinersResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1de3cb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heuristic_miner</th>\n",
       "      <th>inductive_miner</th>\n",
       "      <th>alpha_miner</th>\n",
       "      <th>data_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>execution_time</th>\n",
       "      <td>0.034365</td>\n",
       "      <td>0.026889</td>\n",
       "      <td>0.015153</td>\n",
       "      <td>BPI_2020_InternationalDeclarations</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                heuristic_miner  inductive_miner  alpha_miner  \\\n",
       "execution_time         0.034365         0.026889     0.015153   \n",
       "\n",
       "                                          data_set  \n",
       "execution_time  BPI_2020_InternationalDeclarations  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miner_execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e470eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedResult.to_csv('./results/3 - distributed setup/{}_results.csv'.format('-'.join(file_paths)))\n",
    "# miner_execution_time.to_csv('./results/3 - distributed setup/{}_miner_execution_time.csv'.format('-'.join(file_paths)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
