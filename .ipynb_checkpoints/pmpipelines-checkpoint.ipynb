{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "host=\"bolt://localhost:7687\"\n",
    "user=\"neo4j\"\n",
    "password=\"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Process Mining using pm4py and pyspark\n",
    "# `pmpipelines vs.0.0.2` <font color=\"red\"><-- UPDATE VS. AT EACH COMMIT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the libraries and APIs to be used.\n",
    "\n",
    "- `pickle` for object (de)serialization\n",
    "- `pyspark` spark API in Python\n",
    "- `pm4py` an open source process mining python library\n",
    "- `matplotlib` an open source library for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the correct Neo4J connector .jar _BEFORE_ starting the Spark session & initializing the `SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# [NOTE] change the appropriate directory path for the Neo4j connector .jar:\n",
    "# import NEO4J .jar CONNECTOR FOR PYSPARK/SPARK vs.2\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///usr/local/Cellar/apache-spark@2.4.6/2.4.6/libexec/jars/neo4j-connector-apache-spark_2.11-4.0.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from string import Template\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "from graphviz import Digraph \n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf, broadcast, col, lag, lead, unix_timestamp, collect_set, collect_list, explode, row_number, array_contains, monotonically_increasing_id, size, split, lit, map_concat, map_from_arrays, map_keys, map_values, map_from_entries, struct\n",
    "from pyspark.sql.types import StringType, IntegerType, LongType, StructType, StructField, MapType, ArrayType, ByteType, TimestampType\n",
    "\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`directSuccessionFrequency` is an RDD transformation that is used to transform records into the following form:\n",
    "\n",
    "`ACTIVITY_1 [ \n",
    "    [Predecessor1, Frequency1] .. [PredecessorN, FreqencyN], \n",
    "    [Successor1, Frequency1] .. [SuccessorN, FreqencyN],\n",
    "]`\n",
    "\n",
    "**Such that: `( Predecessor {1,} )->(Activity)->( Successor{0,} )`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparationStartTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directSuccessionFrequency(record):\n",
    "    predecessorSet = set(record[1])\n",
    "    successorSet = set(record[2])\n",
    "    \n",
    "    predecessorOutput = []\n",
    "    successorOutput = []\n",
    "    \n",
    "    for item in predecessorSet:\n",
    "        predecessorOutput.append(tuple([item, record[1].count(item)]))\n",
    "        \n",
    "    for item in successorSet:\n",
    "        successorOutput.append(tuple([item, record[2].count(item)]))\n",
    "    \n",
    "    return (record[0], predecessorOutput, successorOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===========\n",
    "## Spark Session\n",
    "</br>\n",
    "\n",
    "#### <font color='red'> only in MAC OS! --> re-create the `/private/tmp/spark-events` directory </font> via `$ mkdir` command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring spark in order to handle real big data.\n",
    "\n",
    "- `spark.executor.memory`: is the memory amount required for executing the tasks (heap size)\n",
    "\n",
    "\n",
    "- `spark.executor.cores`: is the number of tasks that can be running at the same time by the same executor\n",
    "\n",
    "\n",
    "- `spark.executor.instances`: is the number of the executors on the worker\n",
    "\n",
    "\n",
    "- `spark.driver.memory`: is the memory amount to maintain for the driver to recieve the data\n",
    "\n",
    "\n",
    "- `spark.eventLog.enabled`: is to enable the log history for the job for monitoring\n",
    "\n",
    "\n",
    "- `spark.driver.maxResultSize`: is the total size of serialized result\n",
    "- `spark.sql.execution.arrow.maxRecordsPerBatch`: max number of rows for each Arrow record batch (def. 10K/batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f1909584ed0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf=SparkConf()\n",
    "# [NOTE] the present configs are matching a dual-core i5 PC. Reset the specs for your PC.\n",
    "# [NOTE] adding a custom partitions number to override the Spark default n.200\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"10\") # default 200\n",
    "conf.set(\"spark.executor.memory\",\"4g\") # originally: 10g\n",
    "conf.set(\"spark.executor.cores\", \"10\") # originally: 10\n",
    "conf.set(\"spark.executor.instances\", \"10\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\") # originally: 13g\n",
    "conf.set(\"spark.eventLog.enabled\", \"true\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"4g\") # originally: 15g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the schema manually to improve the read time instead of inferring the schema from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# [NOTE] DATA SCHEMA NOT NEEDED ANYMORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NOTE] Check link for Spark running instance + refresh the browser to see the performance indicators: \n",
    "* browser at `http://172.17.37.79:4040/jobs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NOTE] re-set the appropriate directory path for the file:\n",
    "new_log = spark.read.csv(\"data/BPIC15_1.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the columns to the namings that the miners understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log = new_log.withColumnRenamed(\"org:resource\", \"Resource\")\\\n",
    "        .withColumnRenamed(\"activityNameEN\", \"Activity\")\\\n",
    "        .withColumnRenamed(\"case:concept:name\", \"Case ID\")\\\n",
    "        .withColumnRenamed(\"concept:name\", \"Event ID\")\\\n",
    "        .withColumnRenamed(\"time:timestamp\", \"Timestamp\")\\\n",
    "        .withColumn(\"Costs\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log = new_log.select('Case ID', 'Event ID', 'Timestamp', 'Activity', 'Resource', 'Costs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_log = spark.read.csv(\"./alpha_miner.csv\", header=True, sep=',', schema=schema)\n",
    "new_ts_schema = 'yyyy-MM-dd HH:mm:ss'\n",
    "old_ts_schema = 'dd-MM-yyyy:HH.mm'\n",
    "new_log = new_log.withColumn(\"Timestamp\", unix_timestamp(new_log['Timestamp'], new_ts_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning and windowing by Case ID then sorting by timestamp ascendingly\n",
    "\n",
    "### Then calculating the predecessor and successors for each activity by using lag for predecessor and lead for predecessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directSuccessionWindow = Window.partitionBy(\"Case ID\").orderBy(\"Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"predecessor\",lag(\"Activity\",1).over(directSuccessionWindow))\n",
    "df = df.withColumn(\"successor\",lead(\"Activity\",1).over(directSuccessionWindow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_succession_freq = df.select(\"activity\", \"predecessor\", \"successor\").groupby(\"activity\").agg(collect_list(\"predecessor\").alias(\"predecessor\"), collect_list(\"successor\").alias(\"successor\"))\n",
    "direct_succession = df.select(\"activity\", \"predecessor\", \"successor\").groupby(\"activity\").agg(collect_set(\"predecessor\").alias(\"predecessor\"), collect_set(\"successor\").alias(\"successor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then after finding the predecessor and successor, use the directSuccessionFrequency transformer in order to transform the record into the format mentioned in the documentation of the transformer (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_succession_freq = direct_succession_freq.rdd.map(directSuccessionFrequency).toDF([\"activity\", \"predecessor\", \"successor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = direct_succession_freq.select('activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_result = activities.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", host) \\\n",
    "    .option(\"authentication.type\", \"basic\") \\\n",
    "    .option(\"authentication.basic.username\", user) \\\n",
    "    .option(\"authentication.basic.password\", password) \\\n",
    "    .option(\"query\", \"MERGE (p:Activity {name: event.activity})\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_successors = df.select('activity', 'successor', 'Costs').na.fill(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_successors_grouped = activities_successors.groupby(\"activity\", \"successor\").sum('Costs').withColumnRenamed('sum(Costs)', 'cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_successors_result = activities_successors_grouped.repartition(1).write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", host) \\\n",
    "    .option(\"authentication.type\", \"basic\") \\\n",
    "    .option(\"authentication.basic.username\", user) \\\n",
    "    .option(\"authentication.basic.password\", password) \\\n",
    "    .option(\"query\", '''\n",
    "        MATCH (p:Activity {name: event.activity})\n",
    "        MATCH (c:Activity {name: event.successor})\n",
    "        MERGE (p)-[r:PRODUCES {cost: event.cost}]->(c)\n",
    "        ''') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the query from the generated transformation, thus will be used in neo4j to generate the dfg on that side\n",
    "\n",
    "### The same query is used for incremental updates over the dfg that was stored inside neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check link for Neo4j running instance: \n",
    "* browse at `http://localhost:7474`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending the creation query to neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparationExecutionTime = time.time() - preparationStartTime\n",
    "\n",
    "dfgAlgorithmsStart = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the query that gets the DFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "readQuery = \"MATCH result=(p:Activity)-[r:PRODUCES]->(c:Activity) RETURN p.name as Parent, c.name as Child, r.cost as Frequency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [NOTE] The next cell code is one the biggest performance bottlenecks in the process\n",
    "### now it runs on 4  data partitions (instead of the previous standard Spark 200 ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", host) \\\n",
    "    .option(\"authentication.type\", \"basic\") \\\n",
    "    .option(\"authentication.basic.username\", user) \\\n",
    "    .option(\"authentication.basic.password\", password) \\\n",
    "    .option(\"query\", readQuery) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NOTE] (if needed) to delete the created Neo4j database:\n",
    "\n",
    "`MATCH (n)\n",
    "OPTIONAL MATCH (n)-[r]-()\n",
    "delete n, r`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**^_[NOTE]**_REFACTORED UP TO THIS POINT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the DFG that was fetched from neo4j to the format that pm4py used to understand.\n",
    "\n",
    "```\n",
    "{\n",
    "    (Parent_1, Child_1): Frequency_1,\n",
    "    (Parent_2, Child_2): Frequency_2\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenDFG(dfg):\n",
    "    mydict = {}\n",
    "    for item in dfg:\n",
    "        mydict.update(item)\n",
    "    return mydict\n",
    "\n",
    "flatten_dfg = udf(lambda dfg: flattenDFG(dfg), MapType(StructType([StructField(\"parent\", StringType(), True), StructField(\"child\", StringType(), True)]), IntegerType()))\n",
    "\n",
    "def getDFG(p,c,f):\n",
    "    return {(p,c):f}\n",
    "\n",
    "get_dfg = udf(lambda p,c,f: getDFG(p,c,f), MapType(StructType([StructField(\"parent\", StringType(), True), StructField(\"child\", StringType(), True)]), IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the udfs for each miner\n",
    "\n",
    "**Each UDF takes the DFG as input and returns a pickled object representing the net**\n",
    "\n",
    "*By this approach, we were able to store a different type of object rather than the primitives in pyspark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaMiner(dfg):\n",
    "    net, initial_marking, final_marking = alpha_miner.apply_dfg(dfg)\n",
    "    return str(pickle.dumps({'net': net, 'im': initial_marking, 'fm': final_marking}), encoding=\"latin1\")\n",
    "\n",
    "def heuristicMiner(dfg, threshold):\n",
    "    net, im, fm = heuristics_miner.apply_dfg(dfg, parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: threshold})\n",
    "    return str(pickle.dumps({'net': net, 'im': im, 'fm': fm}), encoding=\"latin1\")\n",
    "\n",
    "def inductiveMiner(dfg, threshold):\n",
    "    net, im, fm = inductive_miner.apply_dfg(dfg, parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: threshold})\n",
    "    return str(pickle.dumps({'net': net, 'im': im, 'fm': fm}), encoding=\"latin1\")\n",
    "\n",
    "alpha_miner_alg = udf(lambda dfg: alphaMiner(dfg))\n",
    "heuristic_miner_alg = udf(lambda dfg, threshold: heuristicMiner(dfg, threshold))\n",
    "inductive_miner_alg = udf(lambda dfg, threshold: inductiveMiner(dfg, threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the DFG that will be passed to each miner in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFG = df.withColumn(\"dfg\", get_dfg(df.Parent, df.Child, df.Frequency)).withColumn(\"threshold\", lit(0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFG.toPandas()\n",
    "DFG = DFG.agg(collect_list('dfg').alias('dfg')).withColumn(\"dfg\", flatten_dfg(\"dfg\")).withColumn(\"threshold\", lit(0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling each miner with different miner algorithm on the same DFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphaMinerDF = DFG.withColumn(\"petrinet\", alpha_miner_alg(\"dfg\"))\n",
    "\n",
    "heuristicMinerDF = DFG.withColumn(\"petrinet\", heuristic_miner_alg(\"dfg\", \"threshold\"))\n",
    "\n",
    "inductiveMinerDF = DFG.withColumn(\"petrinet\", inductive_miner_alg(\"dfg\", \"threshold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgAlgorithmsExecutionTime = time.time() - dfgAlgorithmsStart\n",
    "\n",
    "evaluationStartTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log = new_log.withColumn(\"concept:name\", col(\"Activity\")).withColumnRenamed(\"Case ID\", \"case:concept:name\").withColumnRenamed(\"Timestamp\", \"time:timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the petrinet object from the Dataframe of each algorithm in order to run evaluators on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphaMinerNet = alphaMinerDF.select('petrinet').collect()[0][0]\n",
    "heiristicMinerNet = heuristicMinerDF.select('petrinet').collect()[0][0]\n",
    "inductiveMinerNet = inductiveMinerDF.select('petrinet').collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deserializing the extracted petrinet object of each algorithm in order to be used for evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphapn = pickle.loads(bytes(alphaMinerNet, \"latin1\"))\n",
    "heuristicpn = pickle.loads(bytes(heiristicMinerNet, \"latin1\"))\n",
    "inductivepn = pickle.loads(bytes(inductiveMinerNet, \"latin1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the RDD to pandas\n",
    "### Converting the grouped (simplified) RDD to pandas\n",
    "\n",
    "#### This is primarily done because of the current implementation of the evaluators require the log to be in pandas format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_rdd = new_log.toPandas()\n",
    "merged_rdd = spark.read.csv(\"./merged_1_2_3_4_5.csv\", header=True)\n",
    "merged_rdd = merged_rdd.withColumnRenamed(\"org:resource\", \"Resource\")\\\n",
    "        .withColumnRenamed(\"activityNameEN\", \"Activity\")\\\n",
    "        .withColumn(\"Costs\", lit(1))\n",
    "\n",
    "merged_rdd = merged_rdd.select('case:concept:name', 'concept:name', 'time:timestamp', 'Activity', 'Resource', 'Costs')\n",
    "\n",
    "merged_rdd = merged_rdd.withColumn(\"time:timestamp\", unix_timestamp(merged_rdd['time:timestamp'], new_ts_schema))\n",
    "pandas_rdd = merged_rdd.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_rdd.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the evaluation metrics over each petrinet object\n",
    "\n",
    "- Simplicity\n",
    "- Fitness\n",
    "- Precision\n",
    "- Generalization\n",
    "\n",
    "***And eventually summing up all of the values for all evaluators and labeling it as the score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvaluationMetrics(pandasRdd, petrinetObject):\n",
    "    simplicity = simplicity_evaluator.apply(petrinetObject['net'])\n",
    "    fitness = replay_fitness_evaluator.apply(pandasRdd, petrinetObject['net'], petrinetObject['im'], petrinetObject['fm'], variant=replay_fitness_evaluator.Variants.TOKEN_BASED)[\"log_fitness\"]\n",
    "    precision = precision_evaluator.apply(pandasRdd, petrinetObject['net'], petrinetObject['im'], petrinetObject['fm'], variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)\n",
    "    generalization = generalization_evaluator.apply(pandasRdd, petrinetObject['net'], petrinetObject['im'], petrinetObject['fm'])\n",
    "    \n",
    "    return {\"simplicity\" : simplicity, \n",
    "            \"fitness\": fitness, \n",
    "            \"precision\": precision, \n",
    "            \"generalization\": generalization,\n",
    "            \"score\": simplicity+fitness+precision+generalization\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Alpha Miner Score\")\n",
    "# alphaMinerEvaluation = getEvaluationMetrics(pandas_rdd, alphapn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hueristic Miner Score\")\n",
    "heuristicMinerEvaluation = getEvaluationMetrics(pandas_rdd, heuristicpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inductive Miner Score\")\n",
    "inductiveMinerEvaluation = getEvaluationMetrics(pandas_rdd, inductivepn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluationExecutionTime = time.time() - evaluationStartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluationExecutionTime)\n",
    "print(evaluationStartTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfgAlgorithmsExecutionTime)\n",
    "print(dfgAlgorithmsStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preparationExecutionTime)\n",
    "print(preparationStartTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = ['Simplicity', 'Fitness', 'Precision', 'Generalization', 'Score (total)']\n",
    "# alpha_means = [\n",
    "#     alphaMinerEvaluation['simplicity'], \n",
    "#     alphaMinerEvaluation['fitness'], \n",
    "#     alphaMinerEvaluation['precision'], \n",
    "#     alphaMinerEvaluation['generalization'],\n",
    "#     alphaMinerEvaluation['score']\n",
    "# ]\n",
    "heuristic_means = [\n",
    "    heuristicMinerEvaluation['simplicity'], \n",
    "    heuristicMinerEvaluation['fitness'], \n",
    "    heuristicMinerEvaluation['precision'], \n",
    "    heuristicMinerEvaluation['generalization'],\n",
    "    heuristicMinerEvaluation['score']\n",
    "]\n",
    "inductive_means = [\n",
    "    inductiveMinerEvaluation['simplicity'], \n",
    "    inductiveMinerEvaluation['fitness'], \n",
    "    inductiveMinerEvaluation['precision'], \n",
    "    inductiveMinerEvaluation['generalization'],\n",
    "    inductiveMinerEvaluation['score']\n",
    "]\n",
    "\n",
    "Pos = np.arange(len(labels))  # the label locations\n",
    "width = 0.1  # the width of the bars\n",
    "\n",
    "# plt.bar(Pos - width, alpha_means, width = width, label = 'Alpha Miner')\n",
    "plt.bar(Pos, heuristic_means, width = width, label = 'Heuristic Miner')\n",
    "plt.bar(Pos + width, inductive_means, width = width, label = 'Inductive Miner')\n",
    "\n",
    "plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([1,1,1,1])\n",
    "ax.set_title('Execution time per category')\n",
    "ax.set_ylabel('Time in Seconds')\n",
    "ax.set_xlabel('Execution Category')\n",
    "labels = ['Data Preparation', 'Algorithms', 'Evaluators']\n",
    "values = [preparationExecutionTime, dfgAlgorithmsExecutionTime, evaluationExecutionTime]\n",
    "ax.bar(labels,values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
